{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This notebook is from Pennylane online tutorials: https://pennylane.ai/qml/demos/tutorial_variational_classifier/#iris-classification\n",
    "# Partially reproduced here for training purpose\n",
    "\n",
    "# This cell is added by sphinx-gallery\n",
    "# It can be customized to whatever you like\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational classifier\n",
    "======================\n",
    "\n",
    "::: {.meta}\n",
    ":property=\\\"og:description\\\": Using PennyLane to implement quantum\n",
    "circuits that can be trained from labelled data to classify new data\n",
    "samples. :property=\\\"og:image\\\":\n",
    "<https://pennylane.ai/qml/_static/demonstration_assets//classifier_output_59_0.png>\n",
    ":::\n",
    "\n",
    "::: {.related}\n",
    "tutorial\\_data\\_reuploading\\_classifier Data-reuploading classifier\n",
    "tutorial\\_multiclass\\_classification Multiclass margin classifier\n",
    "ensemble\\_multi\\_qpu Ensemble classification with Rigetti and Qiskit\n",
    "devices\n",
    ":::\n",
    "\n",
    "*Author: Maria Schuld --- Posted: 11 October 2019. Last updated: 11\n",
    "December 2023.*\n",
    "\n",
    "In this tutorial, we show how to use PennyLane to implement variational\n",
    "quantum classifiers - quantum circuits that can be trained from labelled\n",
    "data to classify new data samples. The two examples used are inspired by\n",
    "two of the first papers that proposed variational circuits as supervised\n",
    "machine learning models: [Farhi and Neven\n",
    "(2018)](https://arxiv.org/abs/1802.06002) as well as [Schuld et al.\n",
    "(2018)](https://arxiv.org/abs/1804.00633).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris classification\n",
    "======================\n",
    "\n",
    "We now move on to classifying data points from the Iris dataset, which\n",
    "are no longer simple bitstrings but represented as real-valued vectors.\n",
    "The vectors are 2-dimensional, but we will add some \\\"latent\n",
    "dimensions\\\" and therefore encode inputs into 2 qubits.\n",
    "\n",
    "Quantum and classical nodes\n",
    "---------------------------\n",
    "\n",
    "State preparation is not as simple as when we represent a bitstring with\n",
    "a basis state. Every input x has to be translated into a set of angles\n",
    "which can get fed into a small routine for state preparation. To\n",
    "simplify things a bit, we will work with data from the positive\n",
    "subspace, so that we can ignore signs (which would require another\n",
    "cascade of rotations around the Z-axis).\n",
    "\n",
    "The circuit is coded according to the scheme in [Möttönen, et al.\n",
    "(2004)](https://arxiv.org/abs/quant-ph/0407010), or---as presented for\n",
    "positive vectors only---in [Schuld and Petruccione\n",
    "(2018)](https://link.springer.com/book/10.1007/978-3-319-96424-9). We\n",
    "also decomposed controlled Y-axis rotations into more basic gates,\n",
    "following [Nielsen and Chuang\n",
    "(2010)](http://www.michaelnielsen.org/qcqi/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_angles(x):\n",
    "    beta0 = 2 * np.arcsin(np.sqrt(x[1] ** 2) / np.sqrt(x[0] ** 2 + x[1] ** 2 + 1e-12))\n",
    "    beta1 = 2 * np.arcsin(np.sqrt(x[3] ** 2) / np.sqrt(x[2] ** 2 + x[3] ** 2 + 1e-12))\n",
    "    beta2 = 2 * np.arcsin(np.linalg.norm(x[2:]) / np.linalg.norm(x))\n",
    "\n",
    "    return np.array([beta2, -beta1 / 2, beta1 / 2, -beta0 / 2, beta0 / 2])\n",
    "\n",
    "\n",
    "def state_preparation(a):\n",
    "    qml.RY(a[0], wires=0)\n",
    "\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[1], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[2], wires=1)\n",
    "\n",
    "    qml.PauliX(wires=0)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[3], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(a[4], wires=1)\n",
    "    qml.PauliX(wires=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test if this routine actually works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([0.53896774, 0.79503606, 0.27826503, 0.0], requires_grad=False)\n",
    "ang = get_angles(x)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def test(angles):\n",
    "    state_preparation(angles)\n",
    "\n",
    "    return qml.state()\n",
    "\n",
    "\n",
    "state = test(ang)\n",
    "\n",
    "print(\"x               : \", np.round(x, 6))\n",
    "print(\"angles          : \", np.round(ang, 6))\n",
    "print(\"amplitude vector: \", np.round(np.real(state), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method computed the correct angles to prepare the desired state!\n",
    "\n",
    "> ::: {.note}\n",
    "> ::: {.title}\n",
    "> Note\n",
    "> :::\n",
    ">\n",
    "> The `default.qubit` simulator provides a shortcut to\n",
    "> `state_preparation` with the command `qml.StatePrep(x, wires=[0, 1])`.\n",
    "> On state simulators, this just replaces the quantum state with our\n",
    "> (normalized) input. On hardware, the operation implements more\n",
    "> sophisticated versions of the routine used above.\n",
    "> :::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with only 2 qubits now, we need to update the\n",
    "`layer` function. In addition, we redefine the `cost` function to pass\n",
    "the full batch of data to the state preparation of the circuit\n",
    "simultaneously, a technique similar to NumPy broadcasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def layer(layer_weights):\n",
    "    for wire in range(2):\n",
    "        qml.Rot(*layer_weights[wire], wires=wire)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "\n",
    "\n",
    "def cost(weights, bias, X, Y):\n",
    "    # Transpose the batch of input data in order to make the indexing\n",
    "    # in state_preparation work\n",
    "    predictions = variational_classifier(weights, bias, X.T)\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "We load the Iris data set. There is a bit of preprocessing to do in\n",
    "order to encode the inputs into the amplitudes of a quantum state. We\n",
    "will augment the data points by two so-called \\\"latent dimensions\\\",\n",
    "making the size of the padded data point match the size of the state\n",
    "vector in the quantum device. We then need to normalize the data points,\n",
    "and finally, we translate the inputs x to rotation angles using the\n",
    "`get_angles` function we defined above.\n",
    "\n",
    "Data preprocessing should always be done with the problem in mind; for\n",
    "example, if we do not add any latent dimensions, normalization erases\n",
    "any information on the length of the vectors and classes separated by\n",
    "this feature will not be distinguishable.\n",
    "\n",
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "The Iris dataset can be downloaded\n",
    "`<a href=\"https://raw.githubusercontent.com/XanaduAI/qml/master/_static/demonstration_assets/variational_classifier/data/iris_classes1and2_scaled.txt\"\n",
    "download=parity.txt target=\"_blank\">here</a>`{.interpreted-text\n",
    "role=\"html\"} and should be placed in the subfolder\n",
    "`variational_classifer/data`.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"data/iris_classes1and2_scaled.txt\")\n",
    "X = data[:, 0:2]\n",
    "print(f\"First X sample (original)  : {X[0]}\")\n",
    "\n",
    "# pad the vectors to size 2^2=4 with constant values\n",
    "padding = np.ones((len(X), 2)) * 0.1\n",
    "X_pad = np.c_[X, padding]\n",
    "print(f\"First X sample (padded)    : {X_pad[0]}\")\n",
    "\n",
    "# normalize each input\n",
    "normalization = np.sqrt(np.sum(X_pad**2, -1))\n",
    "X_norm = (X_pad.T / normalization).T\n",
    "print(f\"First X sample (normalized): {X_norm[0]}\")\n",
    "\n",
    "# the angles for state preparation are the features\n",
    "features = np.array([get_angles(x) for x in X_norm], requires_grad=False)\n",
    "print(f\"First features sample      : {features[0]}\")\n",
    "\n",
    "Y = data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These angles are our new features, which is why we have renamed X to\n",
    "\"features\" above. Let's plot the stages of preprocessing and play around\n",
    "with the dimensions (dim1, dim2). Some of them still separate the\n",
    "classes well, while others are less informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0][Y == 1], X[:, 1][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n",
    "plt.scatter(X[:, 0][Y == -1], X[:, 1][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n",
    "plt.title(\"Original data\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "dim1 = 0\n",
    "dim2 = 1\n",
    "plt.scatter(X_norm[:, dim1][Y == 1], X_norm[:, dim2][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n",
    "plt.scatter(X_norm[:, dim1][Y == -1], X_norm[:, dim2][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n",
    "plt.title(f\"Padded and normalised data (dims {dim1} and {dim2})\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "dim1 = 0\n",
    "dim2 = 3\n",
    "plt.scatter(features[:, dim1][Y == 1], features[:, dim2][Y == 1], c=\"b\", marker=\"o\", ec=\"k\")\n",
    "plt.scatter(features[:, dim1][Y == -1], features[:, dim2][Y == -1], c=\"r\", marker=\"o\", ec=\"k\")\n",
    "plt.title(f\"Feature vectors (dims {dim1} and {dim2})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we want to generalize from the data samples. This means that\n",
    "we want to train our model on one set of data and test its performance\n",
    "on a second set of data that has not been used in training. To monitor\n",
    "the generalization performance, the data is split into training and\n",
    "validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_data = len(Y)\n",
    "num_train = int(0.75 * num_data)\n",
    "index = np.random.permutation(range(num_data))\n",
    "feats_train = features[index[:num_train]]\n",
    "Y_train = Y[index[:num_train]]\n",
    "feats_val = features[index[num_train:]]\n",
    "Y_val = Y[index[num_train:]]\n",
    "\n",
    "# We need these later for plotting\n",
    "X_train = X[index[:num_train]]\n",
    "X_val = X[index[num_train:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization\n",
    "============\n",
    "\n",
    "First we initialize the variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_qubits = 2\n",
    "num_layers = 6\n",
    "\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
    "    acc = acc / len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we minimize the cost, using the imported optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(0.01)\n",
    "batch_size = 5\n",
    "\n",
    "# train the variational classifier\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(60):\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "    feats_train_batch = feats_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, feats_train_batch, Y_train_batch)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = np.sign(variational_classifier(weights, bias, feats_train.T))\n",
    "    predictions_val = np.sign(variational_classifier(weights, bias, feats_val.T))\n",
    "\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = accuracy(Y_train, predictions_train)\n",
    "    acc_val = accuracy(Y_val, predictions_val)\n",
    "\n",
    "    if (it + 1) % 2 == 0:\n",
    "        _cost = cost(weights, bias, features, Y)\n",
    "        print(\n",
    "            f\"Iter: {it + 1:5d} | Cost: {_cost:0.7f} | \"\n",
    "            f\"Acc train: {acc_train:0.7f} | Acc validation: {acc_val:0.7f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the continuous output of the variational classifier for the\n",
    "first two dimensions of the Iris data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "cm = plt.cm.RdBu\n",
    "\n",
    "# make data for decision regions\n",
    "xx, yy = np.meshgrid(np.linspace(0.0, 1.5, 30), np.linspace(0.0, 1.5, 30))\n",
    "X_grid = [np.array([x, y]) for x, y in zip(xx.flatten(), yy.flatten())]\n",
    "\n",
    "# preprocess grid points like data inputs above\n",
    "padding = 0.1 * np.ones((len(X_grid), 2))\n",
    "X_grid = np.c_[X_grid, padding]  # pad each input\n",
    "normalization = np.sqrt(np.sum(X_grid**2, -1))\n",
    "X_grid = (X_grid.T / normalization).T  # normalize each input\n",
    "features_grid = np.array([get_angles(x) for x in X_grid])  # angles are new features\n",
    "predictions_grid = variational_classifier(weights, bias, features_grid.T)\n",
    "Z = np.reshape(predictions_grid, xx.shape)\n",
    "\n",
    "# plot decision regions\n",
    "levels = np.arange(-1, 1.1, 0.1)\n",
    "cnt = plt.contourf(xx, yy, Z, levels=levels, cmap=cm, alpha=0.8, extend=\"both\")\n",
    "plt.contour(xx, yy, Z, levels=[0.0], colors=(\"black\",), linestyles=(\"--\",), linewidths=(0.8,))\n",
    "plt.colorbar(cnt, ticks=[-1, 0, 1])\n",
    "\n",
    "# plot data\n",
    "for color, label in zip([\"b\", \"r\"], [1, -1]):\n",
    "    plot_x = X_train[:, 0][Y_train == label]\n",
    "    plot_y = X_train[:, 1][Y_train == label]\n",
    "    plt.scatter(plot_x, plot_y, c=color, marker=\"o\", ec=\"k\", label=f\"class {label} train\")\n",
    "    plot_x = (X_val[:, 0][Y_val == label],)\n",
    "    plot_y = (X_val[:, 1][Y_val == label],)\n",
    "    plt.scatter(plot_x, plot_y, c=color, marker=\"^\", ec=\"k\", label=f\"class {label} validation\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the variational classifier learnt a separating line between\n",
    "the datapoints of the two different classes, which allows it to classify\n",
    "even the unseen validation data with perfect accuracy.\n",
    "\n",
    "About the author\n",
    "================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
